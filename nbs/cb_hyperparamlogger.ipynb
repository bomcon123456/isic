{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.hyperlogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperparamsLoggerCallback\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import copy\n",
    "from itertools import zip_longest\n",
    "from typing import List, Any, Dict, Callable\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Optional\n",
    "\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_warn\n",
    "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
    "\n",
    "\n",
    "class HyperparamsLogger(Callback):\n",
    "    r\"\"\"\n",
    "    Automatically logs learning rate for learning rate schedulers during training.\n",
    "\n",
    "    Args:\n",
    "        logging_interval: set to `epoch` or `step` to log `lr` of all optimizers\n",
    "            at the same interval, set to `None` to log at individual interval\n",
    "            according to the `interval` key of each scheduler. Defaults to ``None``.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> from pytorch_lightning import Trainer\n",
    "        >>> from pytorch_lightning.callbacks import LearningRateLogger\n",
    "        >>> lr_logger = LearningRateLogger(logging_interval='step')\n",
    "        >>> trainer = Trainer(callbacks=[lr_logger])\n",
    "\n",
    "    Logging names are automatically determined based on optimizer class name.\n",
    "    In case of multiple optimizers of same type, they will be named `Adam`,\n",
    "    `Adam-1` etc. If a optimizer has multiple parameter groups they will\n",
    "    be named `Adam/pg1`, `Adam/pg2` etc. To control naming, pass in a\n",
    "    `name` keyword in the construction of the learning rate schdulers\n",
    "\n",
    "    Example::\n",
    "\n",
    "        def configure_optimizer(self):\n",
    "            optimizer = torch.optim.Adam(...)\n",
    "            lr_scheduler = {'scheduler': torch.optim.lr_schedulers.LambdaLR(optimizer, ...)\n",
    "                            'name': 'my_logging_name'}\n",
    "            return [optimizer], [lr_scheduler]\n",
    "    \"\"\"\n",
    "    def __init__(self, logging_interval: Optional[str] = None):\n",
    "        if logging_interval not in (None, 'step', 'epoch'):\n",
    "            raise MisconfigurationException(\n",
    "                'logging_interval should be `step` or `epoch` or `None`.'\n",
    "            )\n",
    "\n",
    "        self.logging_interval = logging_interval\n",
    "        self.lrs = None\n",
    "        self.mms = None\n",
    "        self.lr_sch_names = []\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        \"\"\" Called before training, determines unique names for all lr\n",
    "            schedulers in the case of multiple of the same type or in\n",
    "            the case of multiple parameter groups\n",
    "        \"\"\"\n",
    "        if not trainer.logger:\n",
    "            raise MisconfigurationException(\n",
    "                'Cannot use LearningRateLogger callback with Trainer that has no logger.'\n",
    "            )\n",
    "\n",
    "        if not trainer.lr_schedulers:\n",
    "            rank_zero_warn(\n",
    "                'You are using LearningRateLogger callback with models that'\n",
    "                ' have no learning rate schedulers. Please see documentation'\n",
    "                ' for `configure_optimizers` method.', RuntimeWarning\n",
    "            )\n",
    "\n",
    "        # Find names for schedulers\n",
    "        lr_names = self._find_names(trainer.lr_schedulers)\n",
    "        momentum_names = self._find_names(trainer.lr_schedulers, 'momentum')\n",
    "        # Initialize for storing values\n",
    "        self.lrs = {name: [] for name in lr_names}\n",
    "        self.mms = {name: [] for name in momentum_names}\n",
    "\n",
    "    def on_batch_start(self, trainer, pl_module):\n",
    "        if self.logging_interval != 'epoch':\n",
    "            interval = 'step' if self.logging_interval is None else 'any'\n",
    "            latest_stat = self._extract_hps(trainer, interval)\n",
    "\n",
    "            if trainer.logger is not None and latest_stat:\n",
    "                trainer.logger.log_metrics(latest_stat, step=trainer.global_step)\n",
    "\n",
    "    def on_epoch_start(self, trainer, pl_module):\n",
    "        if self.logging_interval != 'step':\n",
    "            interval = 'epoch' if self.logging_interval is None else 'any'\n",
    "            latest_stat = self._extract_hps(trainer, interval)\n",
    "\n",
    "            if trainer.logger is not None and latest_stat:\n",
    "                trainer.logger.log_metrics(latest_stat, step=trainer.current_epoch)\n",
    "\n",
    "    def _extract_hps(self, trainer, interval):\n",
    "        latest_stat = {}\n",
    "        for scheduler in trainer.lr_schedulers:\n",
    "            for name in self.lr_sch_names:\n",
    "                if scheduler['interval'] == interval or interval == 'any':\n",
    "                    param_groups = scheduler['scheduler'].optimizer.param_groups\n",
    "                    if len(param_groups) != 1:\n",
    "                        for i, pg in enumerate(param_groups):\n",
    "                            key =  f'{name}/pg{i + 1}'\n",
    "                            if 'lr' in name:\n",
    "                                lr = pg['lr']\n",
    "                                self.lrs[key].append(lr)\n",
    "                                latest_stat[key] = lr\n",
    "                            elif 'momentum' in name:\n",
    "                                val = self._get_momentum(pg)\n",
    "                                self.mms[key].append(val)\n",
    "                                latest_stat[key] = val\n",
    "\n",
    "                    else:\n",
    "                        if 'lr' in name:\n",
    "                            self.lrs[name].append(param_groups[0]['lr'])\n",
    "                            latest_stat[name] = param_groups[0]['lr']\n",
    "                        elif 'momentum' in name:\n",
    "                            val = self._get_momentum(param_groups[0])\n",
    "                            self.mms[name].append(val)\n",
    "                            latest_stat[name] = val\n",
    "        return latest_stat\n",
    "\n",
    "    def _get_momentum(self, pg):\n",
    "        val = pg.get('momentum')\n",
    "        if val is None:\n",
    "            val = pg.get('betas')\n",
    "            if val is None:\n",
    "                raise Exception('This optimizer doenst have momentum.')\n",
    "            val = val[0]\n",
    "        return val\n",
    "\n",
    "    def _find_names(self, lr_schedulers, key='lr'):\n",
    "        # Create uniqe names in the case we have multiple of the same learning\n",
    "        # rate schduler + multiple parameter groups\n",
    "\n",
    "        names = []\n",
    "        for scheduler in lr_schedulers:\n",
    "            sch = scheduler['scheduler']\n",
    "            if 'name' in scheduler:\n",
    "                name = scheduler['name']\n",
    "            else:\n",
    "                opt_name = key + '-' + sch.optimizer.__class__.__name__\n",
    "                i, name = 1, opt_name\n",
    "                # Multiple schduler of the same type\n",
    "                while True:\n",
    "                    if name not in names:\n",
    "                        break\n",
    "                    i, name = i + 1, f'{opt_name}-{i}'\n",
    "\n",
    "            # Multiple param groups for the same schduler\n",
    "            param_groups = sch.optimizer.param_groups\n",
    "\n",
    "            if len(param_groups) != 1:\n",
    "                for i, pg in enumerate(param_groups):\n",
    "                    temp = f'{name}/pg{i + 1}'\n",
    "                    names.append(temp)\n",
    "            else:\n",
    "                names.append(name)\n",
    "            self.lr_sch_names.append(name)\n",
    "\n",
    "        return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
