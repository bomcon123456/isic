{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.cutmix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutmix Callback\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.beta import Beta\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_info, rank_zero_warn\n",
    "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
    "\n",
    "from isic.utils import unsqueeze, reduce_loss, NoneReduce\n",
    "from isic.layers import MixLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CutmixDict(Callback):\n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__()\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        assert hasattr(pl_module, 'loss_func'), 'Your LightningModule should have loss_func attribute as your loss function.'\n",
    "        self.old_lf = pl_module.loss_func\n",
    "        self.loss_fnc = MixLoss(self.old_lf, self)\n",
    "        pl_module.loss_func = self.loss_fnc\n",
    "        self.pl_module = pl_module\n",
    "\n",
    "    def _cutmix(self, batch, logger, log_image=False, pre_fix='train'):\n",
    "        xb, yb = batch[\"img\"], batch[\"label\"]\n",
    "        bs = yb.size(0)\n",
    "        W, H = xb.size(3), xb.size(2)\n",
    "\n",
    "        lam = self.distrib.sample((1,)).squeeze()\n",
    "        lam = torch.stack([lam, 1-lam])\n",
    "        self.lam = lam.max()\n",
    "\n",
    "        # Permute the batch\n",
    "        shuffle = torch.randperm(bs)\n",
    "        xb_1, self.yb_1 = xb[shuffle], yb[shuffle]\n",
    "\n",
    "        x1, y1, x2, y2 = self.rand_bbox(W, H, self.lam)\n",
    "        xb[:, :, x1:x2, y1:y2] = xb_1[:, :, x1:x2, y1:y2]\n",
    "        self.lam = (1 - ((x2-x1) * (y2-y1)) / float(W*H))\n",
    "        \n",
    "        if log_image:\n",
    "            grid = torchvision.utils.make_grid(xb)\n",
    "            logger.experiment.add_image(pre_fix + '_cutmix', grid)\n",
    "            grid_g = torchvision.utils.make_grid(xb_1)\n",
    "            logger.experiment.add_image(pre_fix + '_cut_from', grid_g)\n",
    "            dif = abs(xb - xb_1)\n",
    "            grid_d = torchvision.utils.make_grid(dif)\n",
    "            logger.experiment.add_image(pre_fix + '_dif', grid_d)\n",
    "        return xb\n",
    "        \n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n",
    "        x = self._cutmix(batch, trainer.logger)\n",
    "        batch[\"img\"] = x\n",
    "\n",
    "#     def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n",
    "#         x = self._cutmix(batch, trainer.logger, True, 'val')\n",
    "#         batch[\"img\"] = x\n",
    "\n",
    "    def on_validation_start(self, trainer, pl_module):\n",
    "        pl_module.loss_func = self.old_lf\n",
    "    \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        pl_module.loss_func = self.loss_fnc\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        pl_module.loss_func = self.old_lf\n",
    "\n",
    "    def rand_bbox(self, W, H, lam):\n",
    "        cut_rat = torch.sqrt(1. - lam)\n",
    "        cut_w = (W * cut_rat).type(torch.long)\n",
    "        cut_h = (H * cut_rat).type(torch.long)\n",
    "        # uniform\n",
    "        cx = torch.randint(0, W, (1,))\n",
    "        cy = torch.randint(0, H, (1,))\n",
    "        x1 = torch.clamp(cx - cut_w // 2, 0, W)\n",
    "        y1 = torch.clamp(cy - cut_h // 2, 0, H)\n",
    "        x2 = torch.clamp(cx + cut_w // 2, 0, W)\n",
    "        y2 = torch.clamp(cy + cut_h // 2, 0, H)\n",
    "        return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted cb_cutmix.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script('cb_cutmix.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
