{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UtilsModel\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, NullFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "from isic.layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d, sigmoid, sigmoid_, norm_types, cond_init\n",
    "from isic.callback.freeze import FreezeCallback, UnfreezeCallback\n",
    "from isic.utils.core import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_require_grad(p, b):\n",
    "    if getattr(p, 'force_train', False):\n",
    "        p.requires_grad_(True)\n",
    "        return\n",
    "    p.requires_grad_(b)\n",
    "\n",
    "def freeze_to(n, model, n_groups):\n",
    "    frozen_idx = n if n >= 0 else n_groups + n\n",
    "    if frozen_idx >= n_groups:\n",
    "        #TODO use warnings.warn\n",
    "        print(f\"Freezing {frozen_idx} groups; model has {n_groups}; whole model is frozen.\")\n",
    "    for ps in model.get_params(split_bn=False)[n:]:\n",
    "        for p in ps:\n",
    "            # require_grad -> True\n",
    "            set_require_grad(p, True)\n",
    "    for ps in model.get_params(split_bn=False)[:n]:\n",
    "        for p in ps: \n",
    "            # require_grad -> False\n",
    "            set_require_grad(p, False)\n",
    "\n",
    "def freeze(model, n_groups):\n",
    "    assert(n_groups>1)\n",
    "    freeze_to(-1, model, n_groups)\n",
    "    \n",
    "def unfreeze(model, n_groups):\n",
    "    freeze_to(0, model, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_num_ftrs(model, cut):\n",
    "    # TODO: Handle if used models using 1 channel\n",
    "    c_in, h, w = 3, 64, 64\n",
    "    modules = list(model.children())[:cut]\n",
    "    test = nn.Sequential(*modules)\n",
    "    x = torch.rand(1 , c_in, h, w)\n",
    "    out = test.eval()(x)\n",
    "    return out.shape[1]\n",
    "\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return list(m.parameters())\n",
    "\n",
    "def has_pool_type(m):\n",
    "    def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)\n",
    "    \"Return `True` if `m` is a pooling layer or has one in its children\"\n",
    "    if _is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_head(n_in, n_out, lin_ftrs=None, p=0.5, concat_pool=True):\n",
    "    n_in = n_in * (2 if concat_pool else 1)\n",
    "    lin_ftrs = [n_in, 512, n_out] if lin_ftrs is None else [n_in] + lin_ftrs + [n_out]\n",
    "    p_dropouts = [p/2] * (len(lin_ftrs) - 2) + [p]\n",
    "    activations = [nn.ReLU(inplace=True)] * (len(lin_ftrs) - 2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, nn.Flatten()]\n",
    "    for ni, no, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], p_dropouts, activations):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_body(arch):\n",
    "    from isic.hook import num_features_model\n",
    "\n",
    "    def _xresnet_split(m):\n",
    "        return [params(m[0][:3]), params(m[0][3:]), params(m[1:])]\n",
    "    def _resnet_split(m):\n",
    "        return [params(m[0][:6]), params(m[0][6:]), params(m[1:])]\n",
    "    def _squeezenet_split(m):\n",
    "        return [params(m[0][0][:5]), params(m[0][0][5:]), params(m[1:])]\n",
    "    def _densenet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:7]), params(m[0][0][7:]), params(m[1:])]\n",
    "    def _vgg_split(m:nn.Module): \n",
    "        return [params(m[0][0][:22]), params(m[0][0][22:]), params(m[1:])]\n",
    "    def _alexnet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:6]), params(m[0][0][6:]), params(m[1:])]\n",
    "    def _norm_split(m):\n",
    "        return [p for p in m.parameters() if p.requires_grad]\n",
    "\n",
    "    if isinstance(arch, str):\n",
    "        model = getattr(models, arch)(pretrained=True)\n",
    "        if 'xresnet' in arch:\n",
    "            cut = -4\n",
    "            split = _xresnet_split\n",
    "        elif 'resnet' in arch:\n",
    "            cut = -2\n",
    "            split = _resnet_split\n",
    "        elif 'squeeze' in arch:\n",
    "            cut = -1\n",
    "            split = _squeezenet_split\n",
    "        elif 'dense' in arch:\n",
    "            cut = -1\n",
    "            split = _densenet_split\n",
    "        elif 'vgg' in arch:\n",
    "            cut = -2\n",
    "            split = _vgg_split\n",
    "        elif 'alex' in arch:\n",
    "            cut = -2\n",
    "            split = _alexnet_split\n",
    "        else:\n",
    "            ll = list(enumerate(model.children()))\n",
    "            cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "            split = _norm_split\n",
    "        body = nn.Sequential(*list(model.children())[:cut])\n",
    "    else:\n",
    "        model = arch\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "        split = _norm_split\n",
    "        body = nn.Sequential(*list(model.children())[:cut])\n",
    "    num_ftrs = num_features_model(body)\n",
    "\n",
    "    return body, split, num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)\n",
    "\n",
    "# Cell\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bias_batchnorm_params(m, with_bias=True):\n",
    "    \"Return all bias and and BatchNorm params\"\n",
    "    if isinstance(m, norm_types):\n",
    "        return list(m.parameters())\n",
    "    res = []\n",
    "    for c in m.children():\n",
    "        r = get_bias_batchnorm_params(c, with_bias)\n",
    "        res += r\n",
    "    if with_bias and getattr(m, 'bias', None) is not None:\n",
    "        res.append(m.bias)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_grad_block(ms):\n",
    "    \"\"\"\n",
    "        This version still print block module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        r = []\n",
    "        print(m)\n",
    "        for p in m.parameters():\n",
    "            if hasattr(p, 'requires_grad'):\n",
    "                r.append(p.requires_grad)\n",
    "        print(r)\n",
    "\n",
    "\n",
    "def check_attrib_module(ms, attribs=['requires_grad', 'skip_wd']):\n",
    "    \"\"\"\n",
    "        This version only print the smallest module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        if len(list(m.children()))>0:\n",
    "            check_attrib_module(m, attribs)\n",
    "            continue\n",
    "        print(m)\n",
    "        r = []\n",
    "        for name, p in m.named_parameters():\n",
    "            for attr in attribs:\n",
    "                if hasattr(p, attr):\n",
    "                    r.append(name + '-' + attr + '-'+ str(getattr(p, attr)))\n",
    "        print(r)\n",
    "        \n",
    "def get_module_with_attrib(model, attrib='requires_grad'):\n",
    "    for n, p in model.named_parameters():\n",
    "        if getattr(p, attrib, False):\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_lr_loss(lrs, losses):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "    locmin = LogLocator(base=10.0,subs=np.arange(2, 10, 2)*.1,numticks=12)\n",
    "    ax.xaxis.set_minor_locator(locmin)\n",
    "    ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lr_find(model, dm, min_lr=1e-7, max_lr=1., n_train=100, \n",
    "            exp=True, cpu=False, fast_dev_run=False, skip_last=5, verbose=False):\n",
    "    args = {}\n",
    "    lr_finder=None\n",
    "    if not cpu:\n",
    "        args = {\n",
    "            \"gpus\": 1,\n",
    "            \"precision\": 16\n",
    "        }\n",
    "    if fast_dev_run:\n",
    "        trainer = pl.Trainer(fast_dev_run=True, **args)\n",
    "        trainer.fit(model, dm)\n",
    "    else:\n",
    "        trainer = pl.Trainer(max_epochs=1, **args)\n",
    "        lr_finder = trainer.tuner.lr_find(model, dm.train_dataloader(), dm.val_dataloader(), \n",
    "                                    min_lr=min_lr, max_lr=max_lr,\n",
    "                                    num_training=n_train,\n",
    "                                    mode='exponential' if exp else 'linear', early_stop_threshold=None)\n",
    "        \n",
    "        # Inspect results\n",
    "        lrs, losses = lr_finder.results['lr'][:-skip_last], lr_finder.results['loss'][:-skip_last]\n",
    "        fig, ax = plot_lr_loss(lrs, losses)\n",
    "\n",
    "        opt_lr = lr_finder.suggestion()\n",
    "        \n",
    "        ax.plot(lrs[lr_finder._optimal_idx], losses[lr_finder._optimal_idx],\n",
    "                markersize=10, marker='o', color='red')\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Learning Rate\")\n",
    "        print(f'LR suggestion: {opt_lr:e}')\n",
    "\n",
    "    if verbose:\n",
    "        print('Optimizer Information:')\n",
    "        print(trainer.optimizers[0])\n",
    "        print('='*88)\n",
    "        print(('*'*30)+'Check requires_grad/ skip_wd' + ('*'*30))\n",
    "        print(('-'*40)+'    BODY    ' + ('-'*40))\n",
    "        check_attrib_module(model.model[0])\n",
    "        print(('*'*40)+'    HEAD    ' + ('*'*40))\n",
    "        check_attrib_module(model.model[1])\n",
    "        \n",
    "    return lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParameterModule(nn.Module):\n",
    "    \"Register a lone parameter `p` in a module.\"\n",
    "    def __init__(self, p): self.val = p\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=5, out_features=10, bias=True)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a is lone parameter\n",
    "class TstModule(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.a,self.lin = nn.Parameter(torch.randn(1)),nn.Linear(5,10)\n",
    "\n",
    "test = TstModule()\n",
    "list(test.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _has_children(m):\n",
    "    try: next(m.children())\n",
    "    except StopIteration: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def has_params(m):\n",
    "    \"Check if `m` has at least one parameter\"\n",
    "    return len(list(m.parameters())) > 0\n",
    "\n",
    "def total_params(m):\n",
    "    \"Give the number of parameters of a module and if it's trainable or not\"\n",
    "    params = sum([p.numel() for p in m.parameters()])\n",
    "    trains = [p.requires_grad for p in m.parameters()]\n",
    "    return params, (False if len(trains)==0 else trains[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nn.Module.has_children = property(_has_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def children_and_parameters(m):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children\n",
    "\n",
    "def flatten_model(m):\n",
    "    \"Return the list of all submodules and parameters of `m`\"\n",
    "    return sum(map(flatten_model,children_and_parameters(m)),[]) if m.has_children else [m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def in_channels(m):\n",
    "    \"Return the shape of the first weight layer in `m`.\"\n",
    "    for l in flatten_model(m):\n",
    "        if getattr(l, 'weight', None) is not None and l.weight.ndim==4:\n",
    "            return l.weight.shape[1]\n",
    "    raise Exception('No weight layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def one_param(m):\n",
    "    \"First parameter in `m`\"\n",
    "    return first(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def log_metrics_per_key(logger, metrics):\n",
    "    keys = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'mel', 'vasc']\n",
    "    for m_k, v in metrics.items():\n",
    "        for i,k in enumerate(keys):\n",
    "            logger.log(f\"val_{m_k}_{k}\", v[i], prog_bar=True)\n",
    "            logger.log(f\"val_{m_k}_{k}\", v[i], prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs)\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.to(device)\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted utils_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script('utils_model.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
