{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UtilsModel\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bomco\\miniconda3\\envs\\devtorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, NullFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "from isic.layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d\n",
    "from isic.callback.freeze import FreezeCallback, UnfreezeCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_require_grad(p, b):\n",
    "    if getattr(p, 'force_train', False):\n",
    "        p.requires_grad_(True)\n",
    "        return\n",
    "    p.requires_grad_(b)\n",
    "\n",
    "def freeze_to(n, model, n_groups):\n",
    "    frozen_idx = n if n >= 0 else n_groups + n\n",
    "    if frozen_idx >= n_groups:\n",
    "        #TODO use warnings.warn\n",
    "        print(f\"Freezing {frozen_idx} groups; model has {n_groups}; whole model is frozen.\")\n",
    "    for ps in model.get_params(split_bn=False)[n:]:\n",
    "        for p in ps:\n",
    "            # require_grad -> True\n",
    "            set_require_grad(p, True)\n",
    "    for ps in model.get_params(split_bn=False)[:n]:\n",
    "        for p in ps: \n",
    "            # require_grad -> False\n",
    "            set_require_grad(p, False)\n",
    "\n",
    "def freeze(model, n_groups):\n",
    "    assert(n_groups>1)\n",
    "    freeze_to(-1, model, n_groups)\n",
    "    \n",
    "def unfreeze(model, n_groups):\n",
    "    freeze_to(0, model, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_head(n_in, n_out, lin_ftrs=None, p=0.5, concat_pool=True):\n",
    "    n_in = n_in * (2 if concat_pool else 1)\n",
    "    lin_ftrs = [n_in, 512, n_out] if lin_ftrs is None else [n_in] + lin_ftrs + [n_out]\n",
    "    p_dropouts = [p/2] * (len(lin_ftrs) - 2) + [p]\n",
    "    activations = [nn.ReLU(inplace=True)] * (len(lin_ftrs) - 2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, nn.Flatten()]\n",
    "    for ni, no, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], p_dropouts, activations):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return list(m.parameters())\n",
    "\n",
    "def has_pool_type(m):\n",
    "    def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)\n",
    "    \"Return `True` if `m` is a pooling layer or has one in its children\"\n",
    "    if _is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False\n",
    "\n",
    "def create_body(arch):\n",
    "    def _xresnet_split(m):\n",
    "        return [params(m[0][:3]), params(m[0][3:]), params(m[1:])]\n",
    "    def _resnet_split(m):\n",
    "        return [params(m[0][:6]), params(m[0][6:]), params(m[1:])]\n",
    "    def _squeezenet_split(m):\n",
    "        return [params(m[0][0][:5]), params(m[0][0][5:]), params(m[1:])]\n",
    "    def _densenet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:7]), params(m[0][0][7:]), params(m[1:])]\n",
    "    def _vgg_split(m:nn.Module): \n",
    "        return [params(m[0][0][:22]), params(m[0][0][22:]), params(m[1:])]\n",
    "    def _alexnet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:6]), params(m[0][0][6:]), params(m[1:])]\n",
    "\n",
    "    model = getattr(models, arch)(pretrained=True)\n",
    "\n",
    "    if 'xresnet' in arch:\n",
    "        cut = -4\n",
    "        split = _xresnet_split\n",
    "    elif 'resnet' in arch:\n",
    "        cut = -2\n",
    "        split = _resnet_split\n",
    "        num_ftrs = model.fc.in_features\n",
    "    elif 'squeeze' in arch:\n",
    "        cut = -1\n",
    "        split = _squeezenet_split\n",
    "    elif 'dense' in arch:\n",
    "        cut = -1\n",
    "        split = _densenet_split\n",
    "    elif 'vgg' in arch:\n",
    "        cut = -2\n",
    "        split = _vgg_split\n",
    "        num_ftrs = 512\n",
    "    elif 'alex' in arch:\n",
    "        cut = -2\n",
    "        split = _alexnet_split\n",
    "    else:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "        split = params\n",
    "    return nn.Sequential(*list(model.children())[:cut]), split, num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps)>0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm)\n",
    "\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, 'weight'): func(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "    return m\n",
    "\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)\n",
    "\n",
    "# Cell\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bias_batchnorm_params(m, with_bias=True):\n",
    "    \"Return all bias and and BatchNorm params\"\n",
    "    if isinstance(m, norm_types):\n",
    "        return list(m.parameters())\n",
    "    res = []\n",
    "    for c in m.children():\n",
    "        r = get_bias_batchnorm_params(c, with_bias)\n",
    "        res += r\n",
    "    if with_bias and getattr(m, 'bias', None) is not None:\n",
    "        res.append(m.bias)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_grad_block(ms):\n",
    "    \"\"\"\n",
    "        This version still print block module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        r = []\n",
    "        print(m)\n",
    "        for p in m.parameters():\n",
    "            if hasattr(p, 'requires_grad'):\n",
    "                r.append(p.requires_grad)\n",
    "        print(r)\n",
    "\n",
    "\n",
    "def check_attrib_module(ms, attrib='requires_grad'):\n",
    "    \"\"\"\n",
    "        This version only print the smallest module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        if len(list(m.children()))>0:\n",
    "            check_attrib_module(m, attrib)\n",
    "            continue\n",
    "        print(m)\n",
    "        r = []\n",
    "        for p in m.parameters():\n",
    "            if hasattr(p, attrib):\n",
    "                r.append(getattr(p, attrib))\n",
    "        print(r)\n",
    "        \n",
    "def get_module_with_attrib(model, attrib='requires_grad'):\n",
    "    for n, p in model.named_parameters():\n",
    "        if getattr(p, attrib, False):\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lr_find(model, dm, min_lr=1e-8, max_lr=1, n_train=100, exp=True, cpu=True, lr_find=True, verbose=False):\n",
    "    args = {}\n",
    "    lr_finder=None\n",
    "    if not cpu:\n",
    "        args = {\n",
    "            \"gpus\": 1,\n",
    "            \"precision\": 16\n",
    "        }\n",
    "    if lr_find:\n",
    "        trainer = pl.Trainer(max_epochs=1, **args)\n",
    "        lr_finder = trainer.lr_find(model, dm.train_dataloader(), dm.val_dataloader(), \n",
    "                                    min_lr=min_lr, max_lr=max_lr,\n",
    "                                    num_training=n_train,\n",
    "                                    mode='exponential' if exp else 'linear', early_stop_threshold=1e10)\n",
    "        \n",
    "        # Inspect results\n",
    "        lrs, losses = lr_finder.results['lr'], lr_finder.results['loss']\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        locmin = LogLocator(base=10.0,subs=np.arange(2, 10, 2)*.1,numticks=12)\n",
    "        ax.xaxis.set_minor_locator(locmin)\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "        opt_lr = lr_finder.suggestion()\n",
    "\n",
    "        ax.plot(lrs[lr_finder._optimal_idx], losses[lr_finder._optimal_idx],\n",
    "                markersize=10, marker='o', color='red')\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Learning Rate\")\n",
    "        print(f'LR suggestion: {opt_lr:e}')\n",
    "        \n",
    "    else:\n",
    "        trainer = pl.Trainer(max_epochs=1, fast_dev_run=True, **args)\n",
    "        trainer.fit(model, dm)\n",
    "    if verbose:\n",
    "        print(('*'*30)+'Check requires_grad' + ('*'*30))\n",
    "        check_attrib_module(model.model[0])\n",
    "        print('-' * 80)\n",
    "        check_attrib_module(model.model[1])\n",
    "        print(('*'*30)+'Check skip_wd' + ('*'*30))\n",
    "        check_attrib_module(model.model[0], 'skip_wd')\n",
    "        print('-' * 80)\n",
    "        check_attrib_module(model.model[1], 'skip_wd')\n",
    "        \n",
    "    return lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted utils_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script('utils_model.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
