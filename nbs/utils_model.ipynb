{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UtilsModel\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bomco\\miniconda3\\envs\\devtorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, NullFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "from isic.layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d\n",
    "from isic.callback.freeze import FreezeCallback, UnfreezeCallback\n",
    "from isic.utils.core import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_require_grad(p, b):\n",
    "    if getattr(p, 'force_train', False):\n",
    "        p.requires_grad_(True)\n",
    "        return\n",
    "    p.requires_grad_(b)\n",
    "\n",
    "def freeze_to(n, model, n_groups):\n",
    "    frozen_idx = n if n >= 0 else n_groups + n\n",
    "    if frozen_idx >= n_groups:\n",
    "        #TODO use warnings.warn\n",
    "        print(f\"Freezing {frozen_idx} groups; model has {n_groups}; whole model is frozen.\")\n",
    "    for ps in model.get_params(split_bn=False)[n:]:\n",
    "        for p in ps:\n",
    "            # require_grad -> True\n",
    "            set_require_grad(p, True)\n",
    "    for ps in model.get_params(split_bn=False)[:n]:\n",
    "        for p in ps: \n",
    "            # require_grad -> False\n",
    "            set_require_grad(p, False)\n",
    "\n",
    "def freeze(model, n_groups):\n",
    "    assert(n_groups>1)\n",
    "    freeze_to(-1, model, n_groups)\n",
    "    \n",
    "def unfreeze(model, n_groups):\n",
    "    freeze_to(0, model, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_head(n_in, n_out, lin_ftrs=None, p=0.5, concat_pool=True):\n",
    "    n_in = n_in * (2 if concat_pool else 1)\n",
    "    lin_ftrs = [n_in, 512, n_out] if lin_ftrs is None else [n_in] + lin_ftrs + [n_out]\n",
    "    p_dropouts = [p/2] * (len(lin_ftrs) - 2) + [p]\n",
    "    activations = [nn.ReLU(inplace=True)] * (len(lin_ftrs) - 2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, nn.Flatten()]\n",
    "    for ni, no, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], p_dropouts, activations):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_num_ftrs(model, cut):\n",
    "    # TODO: Handle if used models using 1 channel\n",
    "    c_in, h, w = 3, 64, 64\n",
    "    modules = list(model.children())[:cut]\n",
    "    test = nn.Sequential(*modules)\n",
    "    x = torch.rand(1 , c_in, h, w)\n",
    "    out = test.eval()(x)\n",
    "    return out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return list(m.parameters())\n",
    "\n",
    "def has_pool_type(m):\n",
    "    def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)\n",
    "    \"Return `True` if `m` is a pooling layer or has one in its children\"\n",
    "    if _is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False\n",
    "\n",
    "def create_body(arch):\n",
    "    def _xresnet_split(m):\n",
    "        return [params(m[0][:3]), params(m[0][3:]), params(m[1:])]\n",
    "    def _resnet_split(m):\n",
    "        return [params(m[0][:6]), params(m[0][6:]), params(m[1:])]\n",
    "    def _squeezenet_split(m):\n",
    "        return [params(m[0][0][:5]), params(m[0][0][5:]), params(m[1:])]\n",
    "    def _densenet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:7]), params(m[0][0][7:]), params(m[1:])]\n",
    "    def _vgg_split(m:nn.Module): \n",
    "        return [params(m[0][0][:22]), params(m[0][0][22:]), params(m[1:])]\n",
    "    def _alexnet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:6]), params(m[0][0][6:]), params(m[1:])]\n",
    "\n",
    "    model = getattr(models, arch)(pretrained=True)\n",
    "    \n",
    "    if 'xresnet' in arch:\n",
    "        cut = -4\n",
    "        split = _xresnet_split\n",
    "    elif 'resnet' in arch:\n",
    "        cut = -2\n",
    "        split = _resnet_split\n",
    "        num_ftrs = model.fc.in_features\n",
    "    elif 'squeeze' in arch:\n",
    "        cut = -1\n",
    "        split = _squeezenet_split\n",
    "    elif 'dense' in arch:\n",
    "        cut = -1\n",
    "        split = _densenet_split\n",
    "    elif 'vgg' in arch:\n",
    "        cut = -2\n",
    "        split = _vgg_split\n",
    "        num_ftrs = 512\n",
    "    elif 'alex' in arch:\n",
    "        cut = -2\n",
    "        split = _alexnet_split\n",
    "    else:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "        split = params\n",
    "    return nn.Sequential(*list(model.children())[:cut]), split, num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps)>0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm)\n",
    "\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, 'weight'): func(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "    return m\n",
    "\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)\n",
    "\n",
    "# Cell\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bias_batchnorm_params(m, with_bias=True):\n",
    "    \"Return all bias and and BatchNorm params\"\n",
    "    if isinstance(m, norm_types):\n",
    "        return list(m.parameters())\n",
    "    res = []\n",
    "    for c in m.children():\n",
    "        r = get_bias_batchnorm_params(c, with_bias)\n",
    "        res += r\n",
    "    if with_bias and getattr(m, 'bias', None) is not None:\n",
    "        res.append(m.bias)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_grad_block(ms):\n",
    "    \"\"\"\n",
    "        This version still print block module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        r = []\n",
    "        print(m)\n",
    "        for p in m.parameters():\n",
    "            if hasattr(p, 'requires_grad'):\n",
    "                r.append(p.requires_grad)\n",
    "        print(r)\n",
    "\n",
    "\n",
    "def check_attrib_module(ms, attribs=['requires_grad', 'skip_wd']):\n",
    "    \"\"\"\n",
    "        This version only print the smallest module\n",
    "    \"\"\"\n",
    "    for m in ms.children():\n",
    "        if len(list(m.children()))>0:\n",
    "            check_attrib_module(m, attribs)\n",
    "            continue\n",
    "        print(m)\n",
    "        r = []\n",
    "        for name, p in m.named_parameters():\n",
    "            for attr in attribs:\n",
    "                if hasattr(p, attr):\n",
    "                    r.append(name + '-' + attr + '-'+ str(getattr(p, attr)))\n",
    "        print(r)\n",
    "        \n",
    "def get_module_with_attrib(model, attrib='requires_grad'):\n",
    "    for n, p in model.named_parameters():\n",
    "        if getattr(p, attrib, False):\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lr_find(model, dm, min_lr=1e-7, max_lr=10, n_train=100, exp=True, cpu=True, lr_find=True, verbose=False):\n",
    "    args = {}\n",
    "    lr_finder=None\n",
    "    if not cpu:\n",
    "        args = {\n",
    "            \"gpus\": 1,\n",
    "            \"precision\": 16\n",
    "        }\n",
    "    if lr_find:\n",
    "        trainer = pl.Trainer(max_epochs=1, **args)\n",
    "        lr_finder = trainer.lr_find(model, dm.train_dataloader(), dm.val_dataloader(), \n",
    "                                    min_lr=min_lr, max_lr=max_lr,\n",
    "                                    num_training=n_train,\n",
    "                                    mode='exponential' if exp else 'linear', early_stop_threshold=1e10)\n",
    "        \n",
    "        # Inspect results\n",
    "        lrs, losses = lr_finder.results['lr'], lr_finder.results['loss']\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        locmin = LogLocator(base=10.0,subs=np.arange(2, 10, 2)*.1,numticks=12)\n",
    "        ax.xaxis.set_minor_locator(locmin)\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "        opt_lr = lr_finder.suggestion()\n",
    "\n",
    "        ax.plot(lrs[lr_finder._optimal_idx], losses[lr_finder._optimal_idx],\n",
    "                markersize=10, marker='o', color='red')\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Learning Rate\")\n",
    "        print(f'LR suggestion: {opt_lr:e}')\n",
    "        \n",
    "    else:\n",
    "        trainer = pl.Trainer(max_epochs=1, fast_dev_run=True, **args)\n",
    "        trainer.fit(model, dm)\n",
    "    if verbose:\n",
    "        print(trainer.optimizers[0])\n",
    "        print(('*'*30)+'Check requires_grad/ skip_wd' + ('*'*30))\n",
    "        check_attrib_module(model.model[0])\n",
    "        print('-' * 80)\n",
    "        check_attrib_module(model.model[1])\n",
    "        \n",
    "    return lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParameterModule(nn.Module):\n",
    "    \"Register a lone parameter `p` in a module.\"\n",
    "    def __init__(self, p): self.val = p\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=5, out_features=10, bias=True)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a is lone parameter\n",
    "class TstModule(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.a,self.lin = nn.Parameter(torch.randn(1)),nn.Linear(5,10)\n",
    "\n",
    "test = TstModule()\n",
    "list(test.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _has_children(m):\n",
    "    try: next(m.children())\n",
    "    except StopIteration: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def has_params(m):\n",
    "    \"Check if `m` has at least one parameter\"\n",
    "    return len(list(m.parameters())) > 0\n",
    "\n",
    "def total_params(m):\n",
    "    \"Give the number of parameters of a module and if it's trainable or not\"\n",
    "    params = sum([p.numel() for p in m.parameters()])\n",
    "    trains = [p.requires_grad for p in m.parameters()]\n",
    "    return params, (False if len(trains)==0 else trains[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nn.Module.has_children = property(_has_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def children_and_parameters(m):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children\n",
    "\n",
    "def flatten_model(m):\n",
    "    \"Return the list of all submodules and parameters of `m`\"\n",
    "    return sum(map(flatten_model,children_and_parameters(m)),[]) if m.has_children else [m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def in_channels(m):\n",
    "    \"Return the shape of the first weight layer in `m`.\"\n",
    "    for l in flatten_model(m):\n",
    "        if getattr(l, 'weight', None) is not None and l.weight.ndim==4:\n",
    "            return l.weight.shape[1]\n",
    "    raise Exception('No weight layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def one_param(m):\n",
    "    \"First parameter in `m`\"\n",
    "    return first(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted utils_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script('utils_model.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
