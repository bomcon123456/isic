{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup Callback\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bomco\\miniconda3\\envs\\devtorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.beta import Beta\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_info, rank_zero_warn\n",
    "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
    "\n",
    "from isic.utils import unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NoneReduce():\n",
    "    \"A context manager to evaluate `loss_func` with none reduce.\"\n",
    "    def __init__(self, loss_func): self.loss_func,self.old_red = loss_func,None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if hasattr(self.loss_func, 'reduction'):\n",
    "            self.old_red = self.loss_func.reduction\n",
    "            self.loss_func.reduction = 'none'\n",
    "            return self.loss_func\n",
    "        else: return partial(self.loss_func, reduction='none')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.old_red is not None: self.loss_func.reduction = self.old_red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Mixup(Callback):\n",
    "    r\"\"\"\n",
    "    Mixup\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.4):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        assert hasattr(pl_module, 'loss_func'), 'Your LightningModule should have loss_func attribute as your loss function.'\n",
    "        self.old_lf, pl_module.loss_func = pl_module.loss_func, self.loss_func\n",
    "        self.pl_module = pl_module\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n",
    "        device = pl_module.device\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        bs = y.size(0)\n",
    "\n",
    "        # Produce \"bs\" probability for each sample\n",
    "        lam = self.distrib.sample((bs,)).squeeze().to(device)\n",
    "\n",
    "        # Get those probability that >0.5, so that the first img (in the nonshuffle batch) has bigger coeff\n",
    "        # Which avoid duplication mixup\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "\n",
    "        # Permute the batch\n",
    "        shuffle = torch.randperm(bs).to(device)\n",
    "        x1, self.yb1 = x[shuffle], y[shuffle]\n",
    "        nx_dims = len(x.size())\n",
    "        x_new = torch.lerp(x1, x, weight=unsqueeze(self.lam, n=nx_dims-1))\n",
    "        \n",
    "#         grid = torchvision.utils.make_grid(x_new)\n",
    "#         trainer.logger.experiment.add_image('mixup', grid)\n",
    "\n",
    "        pl_module.enhanced_batch = (x_new, y)\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        pl_module.loss_func = self.old_lf\n",
    "\n",
    "    def loss_func(self, pred, yb):\n",
    "        #TODO check if training\n",
    "        if self.pl_module.testing: return self.old_lf(pred, yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred, self.yb1), lf(pred,yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixupDict(Callback):\n",
    "    def __init__(self, alpha=0.4):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        assert hasattr(pl_module, 'loss_func'), 'Your LightningModule should have loss_func attribute as your loss function.'\n",
    "        self.old_lf, pl_module.loss_func = pl_module.loss_func, self.loss_func\n",
    "        self.pl_module = pl_module\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n",
    "        xb, yb = batch[\"img\"], batch[\"label\"]\n",
    "        bs = yb.size(0)\n",
    "\n",
    "        # Produce \"bs\" probability for each sample\n",
    "        lam = self.distrib.sample((bs,)).squeeze()\n",
    "\n",
    "        # Get those probability that >0.5, so that the first img (in the nonshuffle batch) has bigger coeff\n",
    "        # Which avoid duplication mixup\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "\n",
    "        # Permute the batch\n",
    "        shuffle = torch.randperm(bs)\n",
    "        xb_1, self.yb_1 = xb[shuffle], yb[shuffle]\n",
    "        nx_dims = len(xb.size())\n",
    "        weight = unsqueeze(self.lam, n=nx_dims-1)\n",
    "        x_new = torch.lerp(xb_1, xb, weight=weight)\n",
    "        grid = torchvision.utils.make_grid(x_new)\n",
    "        trainer.logger.experiment.add_image('mixup', grid)\n",
    "        grid_g = torchvision.utils.make_grid(xb)\n",
    "        trainer.logger.experiment.add_image('norm', grid_g)\n",
    "        batch[\"img\"] = x_new\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        pl_module.loss_func = self.old_lf\n",
    "\n",
    "    def loss_func(self, pred, yb):\n",
    "        #TODO check if training\n",
    "        if self.pl_module.testing: return self.old_lf(pred, yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            self.yb_1 = self.yb_1.to(pred.device)\n",
    "            self.lam = self.lam.to(pred.device)\n",
    "            loss = torch.lerp(lf(pred, self.yb_1), lf(pred,yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
