{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics import functional as FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from isic.dataset import SkinDataModule\n",
    "from isic.layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d\n",
    "from isic.callback.hyperlogger import HyperparamsLogger\n",
    "from isic.callback.logtable import LogTableMetricsCallback\n",
    "from isic.callback.mixup import MixupDict\n",
    "from isic.callback.cutmix import CutmixDict\n",
    "from isic.callback.freeze import FreezeCallback, UnfreezeCallback\n",
    "from isic.utils import reduce_loss, apply_init, get_bias_batchnorm_params, apply_leaf, print_grad_module, generate_val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_head(n_in, n_out, lin_ftrs=None, p=0.5, concat_pool=True):\n",
    "    n_in = n_in * (2 if concat_pool else 1)\n",
    "    lin_ftrs = [n_in, 512, n_out] if lin_ftrs is None else [n_in] + lin_ftrs + [n_out]\n",
    "    p_dropouts = [p/2] * (len(lin_ftrs) - 2) + [p]\n",
    "    activations = [nn.ReLU(inplace=True)] * (len(lin_ftrs) - 2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, nn.Flatten()]\n",
    "    for ni, no, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], p_dropouts, activations):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return list(m.parameters())\n",
    "\n",
    "def has_pool_type(m):\n",
    "    def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)\n",
    "    \"Return `True` if `m` is a pooling layer or has one in its children\"\n",
    "    if _is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False\n",
    "\n",
    "def create_body(arch):\n",
    "    def _xresnet_split(m):\n",
    "        return [params(m[0][:3]), params(m[0][3:]), params(m[1:])]\n",
    "    def _resnet_split(m):\n",
    "        return [params(m[0][:6]), params(m[0][6:]), params(m[1:])]\n",
    "    def _squeezenet_split(m):\n",
    "        return [params(m[0][0][:5]), params(m[0][0][5:]), params(m[1:])]\n",
    "    def _densenet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:7]), params(m[0][0][7:]), params(m[1:])]\n",
    "    def _vgg_split(m:nn.Module): \n",
    "        return [params(m[0][0][:22]), params(m[0][0][22:]), params(m[1:])]\n",
    "    def _alexnet_split(m:nn.Module): \n",
    "        return [params(m[0][0][:6]), params(m[0][0][6:]), params(m[1:])]\n",
    "\n",
    "    model = getattr(models, arch)(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if 'xresnet' in arch:\n",
    "        cut = -4\n",
    "        split = _xresnet_split\n",
    "    elif 'resnet':\n",
    "        cut = -2\n",
    "        split = _resnet_split\n",
    "    elif 'squeeze':\n",
    "        cut = -1\n",
    "        split = _squeezenet_split\n",
    "    elif 'dense':\n",
    "        cut = -1\n",
    "        split = _densenet_split\n",
    "    elif 'vgg':\n",
    "        cut = -2\n",
    "        split = _vgg_split\n",
    "    elif 'alex':\n",
    "        cut = -2\n",
    "        split = _alexnet_split\n",
    "    else:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "        split = params\n",
    "    return nn.Sequential(*list(model.children())[:cut]), split, num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Model(LightningModule):\n",
    "    def __init__(self, steps_epoch, epochs=30, lr=1e-2, wd=0., n_out=7, concat_pool=True, arch='resnet50'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create body\n",
    "        body, self.split, num_ftrs = create_body(arch)\n",
    "        # create head\n",
    "        head = create_head(num_ftrs, n_out)\n",
    "        \n",
    "        #model\n",
    "        self.model = nn.Sequential(body, head)\n",
    "        apply_init(self.model[1])\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.split(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['img'], batch['label']\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        acc = FM.accuracy(y_hat, y, num_classes=7)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss)\n",
    "        result.log('train_acc', acc, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['img'], batch['label']\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        acc = FM.accuracy(y_hat, y, num_classes=7)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss, prog_bar=True) \n",
    "        result.log('val_acc', acc, prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def create_opt(self, lr):\n",
    "        def _inner():\n",
    "            print('override_called')\n",
    "            param_groups = self.get_params()\n",
    "            n_groups = len(param_groups)\n",
    "            lrs = generate_val_steps(lr, n_groups)\n",
    "            assert len(lrs) == n_groups, f\"Trying to set {len(lrs)} values for LR but there are {n_groups} parameter groups.\"\n",
    "            grps = []\n",
    "            for i in range(n_groups):\n",
    "                grps.append({\n",
    "                    \"params\": param_groups[i],\n",
    "                    \"lr\": lrs[i]\n",
    "                })\n",
    "            print(lrs)\n",
    "            opt = torch.optim.Adam(grps, \n",
    "                        lr=1e-2, weight_decay=self.hparams.wd\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lrs, steps_per_epoch=self.hparams.steps_epoch, epochs=self.hparams.epochs)\n",
    "            sched = {\n",
    "                'scheduler': scheduler, # The LR schduler\n",
    "                'interval': 'step', # The unit of the scheduler's step size\n",
    "                'frequency': 1, # The frequency of the scheduler\n",
    "                'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler\n",
    "            }\n",
    "            opt.t_state={}\n",
    "            for p in get_bias_batchnorm_params(self.model):\n",
    "                opt.t_state[p] = {\"force_train\": True}\n",
    "            return [opt], [sched]\n",
    "        self.configure_optimizers = _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_formater = \"You have set {0} number of classes if different from predicted {0} and target {0} number of classes\"\n",
    "warnings.filterwarnings(\"ignore\", message_formater.format(\"(.*)\"), category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SkinDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "STEPS_EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = Model(steps_epoch=STEPS_EPOCH, epochs=EPOCHS, lr=1e-2)\n",
    "model.create_opt(slice(1e-3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Freeze training\n",
    "hp_log = HyperparamsLogger()\n",
    "freeze_cb = FreezeCallback()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, callbacks=[LogTableMetricsCallback(), hp_log, freeze_cb], fast_dev_run=True, limit_val_batches=0, limit_train_batches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bomco\\miniconda3\\envs\\devtorch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 25 M  \n",
      "1 | loss_func | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "override_called\n",
      "[0.001, 0.03162277660168379, 1.0]\n",
      "wtf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80140ec3a14435f865fc919573245d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.252</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>78.051300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    base_momentum: 0.85\n",
       "    betas: (0.8999999999999999, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 4e-05\n",
       "    lr: 0.0005200000000000001\n",
       "    max_lr: 0.001\n",
       "    max_momentum: 0.95\n",
       "    min_lr: 4e-09\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    base_momentum: 0.85\n",
       "    betas: (0.8999999999999999, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 0.0012649110640673518\n",
       "    lr: 0.016443843832875574\n",
       "    max_lr: 0.03162277660168379\n",
       "    max_momentum: 0.95\n",
       "    min_lr: 1.2649110640673517e-07\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    base_momentum: 0.85\n",
       "    betas: (0.8999999999999999, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 0.04\n",
       "    lr: 0.52\n",
       "    max_lr: 1.0\n",
       "    max_momentum: 0.95\n",
       "    min_lr: 4e-06\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.optimizers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze training\n",
    "hp_log = HyperparamsLogger()\n",
    "freeze_cb = FreezeCallback()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, callbacks=[LogTableMetricsCallback(), hp_log, unfreeze_cb], fast_dev_run=True, limit_val_batches=0, limit_train_batches=0.01)\n",
    "model.create_opt(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 25 M  \n",
      "1 | loss_func | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "override_called\n",
      "wtf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3447eb291f94446941044767a951eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.256</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>4.279556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveAvgPool2d(output_size=1)\n",
      "[]\n",
      "AdaptiveMaxPool2d(output_size=1)\n",
      "[]\n",
      "Flatten()\n",
      "[]\n",
      "BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "[True, True]\n",
      "Dropout(p=0.25, inplace=False)\n",
      "[]\n",
      "Linear(in_features=4096, out_features=512, bias=False)\n",
      "[True]\n",
      "ReLU(inplace=True)\n",
      "[]\n",
      "BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "[True, True]\n",
      "Dropout(p=0.5, inplace=False)\n",
      "[]\n",
      "Linear(in_features=512, out_features=7, bias=False)\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "print_grad_module(model.model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4636), started 4 days, 18:33:26 ago. (Use '!kill 4636' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7752bba96d9fd24c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7752bba96d9fd24c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# class ResnetModel(LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "# #         self.save_hyperparameters()\n",
    "#         self.resnet = models.resnet50(pretrained=True)\n",
    "#         num_ftrs = self.resnet.fc.in_features\n",
    "#         self.resnet.fc = nn.Linear(num_ftrs, 7)\n",
    "#         self.loss_func = F.cross_entropy\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.resnet(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         print(batch)\n",
    "#         if hasattr(self, 'enhanced_batch'):\n",
    "#             print('hehe')\n",
    "#             batch = self.enhanced_batch\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "#         loss = self.loss_func(y_hat, y)\n",
    "#         acc = FM.accuracy(y_hat, y, num_classes=7)\n",
    "#         result = pl.TrainResult(minimize=loss)\n",
    "#         result.log('train_loss', loss)\n",
    "#         result.log('train_acc', acc, prog_bar=True)\n",
    "#         return result\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "#         loss = F.cross_entropy(y_hat, y)\n",
    "#         acc = FM.accuracy(y_hat, y, num_classes=7)\n",
    "#         result = pl.EvalResult(checkpoint_on=loss)\n",
    "#         result.log('val_loss', loss, prog_bar=True) \n",
    "#         result.log('val_acc', acc, prog_bar=True)\n",
    "#         return result\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         opt = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "#         scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=1e-2, steps_per_epoch=140, epochs=10)\n",
    "#         sched = {\n",
    "#             'scheduler': scheduler, # The LR schduler\n",
    "#             'interval': 'step', # The unit of the scheduler's step size\n",
    "#             'frequency': 1, # The frequency of the scheduler\n",
    "#             'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler\n",
    "#         }\n",
    "#         return [opt], [sched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted cb_mixup.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script('model.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
