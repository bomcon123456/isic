# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/cb_freeze.ipynb (unless otherwise specified).

__all__ = ['set_require_grad', 'freeze_to', 'freeze', 'unfreeze', 'FreezeCallback', 'UnfreezeCallback']

# Cell
import torch.nn as nn

from functools import partial
from collections import Iterable
from collections.abc import Generator
import re

import torch
from pytorch_lightning.callbacks import Callback

# Cell
def set_require_grad(p, opt, b):
    if hasattr(opt, 't_state') and p in opt.t_state and opt.t_state[p].get('force_train', False):
        p.requires_grad_(True)
        return
    p.requires_grad_(b)

def freeze_to(n, model, opt):
    n_grps = len(opt.param_groups)
    frozen_idx = n if n >= 0 else n_grps + n
    if frozen_idx >= n_grps:
        #TODO use warnings.warn
        print(f"Freezing {frozen_idx} groups; model has {n_groups}; whole model is frozen.")
    for ps in model.get_params()[n:]:
        for p in ps:
            # require_grad -> True
            set_require_grad(p, opt, True)
    for ps in model.get_params()[:n]:
        for p in ps:
            # require_grad -> False
            set_require_grad(p, opt, False)

def freeze(model, opt):
    assert(len(opt.param_groups)>1)
    freeze_to(-1, model, opt)

def unfreeze(model, opt):
    freeze_to(0, model, opt)

# Cell
class FreezeCallback(Callback):
    def on_train_start(self, trainer, pl_module):
        freeze(pl_module, trainer.optimizers[0])

class UnfreezeCallback(Callback):
    def on_train_start(self, trainer, pl_module):
        unfreeze(pl_module, trainer.optimizers[0])