# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/layers.ipynb (unless otherwise specified).

__all__ = ['AdaptiveConcatPool2d', 'MixLoss', 'LabelSmoothingCrossEntropy', 'LinBnDrop']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F

from .utils import reduce_loss, NoneReduce

# Cell
class AdaptiveConcatPool2d(nn.Module):
    def __init__(self, sz=1):
        super().__init__()
        self.output_size = sz
        self.ap = nn.AdaptiveAvgPool2d(sz)
        self.mp = nn.AdaptiveMaxPool2d(sz)

    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)

# Cell
class MixLoss(nn.Module):
    def __init__(self, old_lf, mixup_cb):
        super().__init__()
        self.old_lf = old_lf
        self.mixup_cb = mixup_cb

    def forward(self, pred, yb):
        if self.mixup_cb.pl_module.testing: return self.old_lf(pred, yb)
        with NoneReduce(self.old_lf) as lf:
            self.mixup_cb.yb_1 = self.mixup_cb.yb_1.to(pred.device)
            self.mixup_cb.lam = self.mixup_cb.lam.to(pred.device)
            loss = torch.lerp(lf(pred, self.mixup_cb.yb_1), lf(pred,yb), self.mixup_cb.lam)
        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))

# Cell
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, eps:float=0.1, reduction='mean'):
        super().__init__()
        self.eps, self.reduction = eps, reduction

    def forward(self, output, target):
        c = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
        nll = F.nll_loss(log_preds, target, reduction=self.reduction)
        return torch.lerp(nll, loss/c, self.eps)

# Cell
def _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):
    "Norm layer with `nf` features and `ndim` initialized depending on `norm_type`."
    assert 1 <= ndim <= 3
    bn = getattr(nn, f"{prefix}{ndim}d")(nf, **kwargs)
    if bn.affine:
        bn.bias.data.fill_(1e-3)
        bn.weight.data.fill_(0. if zero else 1.)
    return bn

class LinBnDrop(nn.Sequential):
    "Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers"
    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):
        layers = [_get_norm('BatchNorm', n_out if lin_first else n_in,
                            ndim=1)] if bn else []
        if p != 0: layers.append(nn.Dropout(p))
        lin = [nn.Linear(n_in, n_out, bias=not bn)]
        if act is not None: lin.append(act)
        layers = lin+layers if lin_first else layers+lin
        super().__init__(*layers)