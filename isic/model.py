# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/model.ipynb (unless otherwise specified).

__all__ = ['ResnetModel']

# Cell
import warnings

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

import pytorch_lightning as pl
from pytorch_lightning.core import LightningModule
from pytorch_lightning.metrics import functional as FM

# Cell
from .dataset import SkinDataModule
from .layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d
from .callback.hyperlogger import HyperparamsLogger
from .callback.logtable import LogTableMetricsCallback
from .callback.mixup import MixupDict
from .callback.cutmix import CutmixDict
from .utils import reduce_loss, apply_init

# Cell
class ResnetModel(LightningModule):
    def __init__(self, steps_epoch, epochs=30, lr=1e-2, wd=0.9, n_out=7, concat_pool=True):
        super().__init__()
        self.save_hyperparameters()
        mdl = models.resnet50(pretrained=True)
        # create body
        cut = -2

        body = nn.Sequential(*list(mdl.children())[:cut])
        # create head
        num_ftrs = mdl.fc.in_features * (2 if concat_pool else 1)
        lin_ftrs = [num_ftrs, 512, n_out]
        p = 0.5
        ps = [p/2] * (len(lin_ftrs) - 2) + [p]
        actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs) - 2) + [None]
        pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)
        layers = [pool, nn.Flatten()]
        for ni, no, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):
            layers += LinBnDrop(ni, no, bn=True, p=p, act=actn)

        head = nn.Sequential(*layers)
        #model
        self.model = nn.Sequential(body, head)
        apply_init(self.model[1])

        self.loss_func = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch['img'], batch['label']
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = FM.accuracy(y_hat, y, num_classes=7)
        result = pl.TrainResult(minimize=loss)
        result.log('train_loss', loss)
        result.log('train_acc', acc, prog_bar=True)
        return result

    def validation_step(self, batch, batch_idx):
        x, y = batch['img'], batch['label']
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = FM.accuracy(y_hat, y, num_classes=7)
        result = pl.EvalResult(checkpoint_on=loss)
        result.log('val_loss', loss, prog_bar=True)
        result.log('val_acc', acc, prog_bar=True)
        return result

    def configure_optimizers(self):
        opt = torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=self.hparams.wd)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=self.hparams.lr, steps_per_epoch=self.hparams.steps_epoch, epochs=self.hparams.epochs)
        sched = {
            'scheduler': scheduler, # The LR schduler
            'interval': 'step', # The unit of the scheduler's step size
            'frequency': 1, # The frequency of the scheduler
            'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler
        }
        return [opt], [sched]