# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/model.ipynb (unless otherwise specified).

__all__ = ['params', 'has_pool_type', 'create_body', 'ResnetModel']

# Cell
import warnings
import re

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

import pytorch_lightning as pl
from pytorch_lightning.core import LightningModule
from pytorch_lightning.metrics import functional as FM

# Cell
from .dataset import SkinDataModule
from .layers import LabelSmoothingCrossEntropy, LinBnDrop, AdaptiveConcatPool2d
from .callback.hyperlogger import HyperparamsLogger
from .callback.logtable import LogTableMetricsCallback
from .callback.mixup import MixupDict
from .callback.cutmix import CutmixDict
from .utils import reduce_loss, apply_init

# Cell
def params(m):
    "Return all parameters of `m`"
    return [p for p in m.parameters()]

def has_pool_type(m):
    def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)
    "Return `True` if `m` is a pooling layer or has one in its children"
    if _is_pool_type(m): return True
    for l in m.children():
        if has_pool_type(l): return True
    return False

def create_body(arch):
    def _xresnet_split(m):
        return [params(m[0][:3]), params(m[0][3:]), params(m[1:])]
    def _resnet_split(m):
        return [params(m[0][:6]), params(m[0][6:]), params(m[1:])]
    def _squeezenet_split(m):
        return [params(m[0][0][:5]), params(m[0][0][5:]), params(m[1:])]
    def _densenet_split(m:nn.Module):
        return [params(m[0][0][:7]), params(m[0][0][7:]), params(m[1:])]
    def _vgg_split(m:nn.Module):
        return [params(m[0][0][:22]), params(m[0][0][22:]), params(m[1:])]
    def _alexnet_split(m:nn.Module):
        return [params(m[0][0][:6]), params(m[0][0][6:]), params(m[1:])]

    model = getattr(models, arch)(pretrained=True)
    num_ftrs = model.fc.in_features
    if 'xresnet' in arch:
        cut = -4
        split = _xresnet_split
    elif 'resnet':
        cut = -2
        split = _xresnet_split
    elif 'squeeze':
        cut = -1
        split = _squeezenet_split
    elif 'dense':
        cut = -1
        split = _densenet_split
    elif 'vgg':
        cut = -2
        split = _vgg_split
    elif 'alex':
        cut = -2
        split = _alexnet_split
    else:
        ll = list(enumerate(model.children()))
        cut = next(i for i,o in reversed(ll) if has_pool_type(o))
        split = params
    return nn.Sequential(*list(model.children())[:cut]), split, num_ftrs

# Cell
class ResnetModel(LightningModule):
    def __init__(self, steps_epoch, epochs=30, lr=1e-2, wd=0.9, n_out=7, concat_pool=True):
        super().__init__()
        self.save_hyperparameters()
        # create body
        body, split, num_ftrs = create_body('resnet50')
        # create head
        head = create_head(num_ftrs, n_out)

        #model
        self.model = nn.Sequential(body, head)
        apply_init(self.model[1])

        self.loss_func = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch['img'], batch['label']
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = FM.accuracy(y_hat, y, num_classes=7)
        result = pl.TrainResult(minimize=loss)
        result.log('train_loss', loss)
        result.log('train_acc', acc, prog_bar=True)
        return result

    def validation_step(self, batch, batch_idx):
        x, y = batch['img'], batch['label']
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = FM.accuracy(y_hat, y, num_classes=7)
        result = pl.EvalResult(checkpoint_on=loss)
        result.log('val_loss', loss, prog_bar=True)
        result.log('val_acc', acc, prog_bar=True)
        return result

    def configure_optimizers(self):
        opt = torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=self.hparams.wd)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=self.hparams.lr, steps_per_epoch=self.hparams.steps_epoch, epochs=self.hparams.epochs)
        sched = {
            'scheduler': scheduler, # The LR schduler
            'interval': 'step', # The unit of the scheduler's step size
            'frequency': 1, # The frequency of the scheduler
            'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler
        }
        return [opt], [sched]